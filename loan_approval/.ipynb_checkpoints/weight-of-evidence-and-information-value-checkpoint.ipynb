{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<div style=\"padding: 115px;color:white;margin:10;border-radius:8px;overflow:hidden;background-image: url(https://www.cimbbank.com.ph/en/financial-literacy-articles/financial-essentials/4-things-you-can-do-with-a-personal-loan/_jcr_content/root/responsivegrid_58989/responsivegrid_470203168/responsivegrid/responsivegrid/image.img.jpeg/1641542565680/loan.jpeg);background-position: 9% 25%\"></div>","metadata":{}},{"cell_type":"markdown","source":"<p style=\"padding-bottom: 10px;\n          margin-bottom: 10px;\n          font-size: 50px;\n          font-weight: bold;\n          color: black; \n          text-align: left;\n          font-family: Poppins\"><b>Loan Approval | Weight of Evidence (WoE) &amp; Information Value (IV)</b>\n<p style=\"font-size: 22px; color: gray; text-align: left;\">Let's delve deeper into two important techniques for feature selection and handling of <i>noisy</i> data</p>\n<hr style=\"height:2px;border-width:0;color:gray;background-color:gray;box-shadow: 0px 2.5px 5px rgba(0, 0, 0, 0.2);\">","metadata":{}},{"cell_type":"markdown","source":"<h3 style=\"border-bottom: 1px solid #ccc;\n            padding-bottom: 10px;\n            margin-bottom: 10px;\n            font-size: 16px;\n            font-weight: bold;\n            color: black;\">Table of Contents</h3>\n    \n- [Introduction](#intro)<br><br>\n- [Exploratory Data Analysis (EDA)](#eda)<br><br>\n- [Weight of Evidence & Information Value](#woe_and_iv)<br><br>\n- [Modeling](#model)<br><br>","metadata":{}},{"cell_type":"markdown","source":"<h1 id = 'intro' style=\"border-bottom: 1px solid #ccc;\n                        padding-bottom: 10px;\n                        margin-bottom: 10px;\n                        font-size: 38px;\n                        font-weight: bold;\n                        color: black;\n                        font-family: Poppins\">Introduction</h1>","metadata":{}},{"cell_type":"markdown","source":"<span style=\"font-size: 20px\">The <i><b>Weight of Evidence (WoE)</b></i> and the <i><b>Information Value (IV)</b></i> are two of the most widely used tools in <mark><b>Finance</b></mark> and <mark><b>credit risk analysis</b></mark>. Not only these tools are extremely relevant for the measurement of predictive power in $Independent$ $Variables$, but also extremely effective in handling outliers, missing values, and categorical data transformation. <br><br>\nIn this notebook, we will explore the practical applications of both WoE and IV by utilizing the <a href=\"https://www.kaggle.com/datasets/architsharma01/loan-approval-prediction-dataset\\\">Loan-Approval-Prediction-Dataset</a>. Our aim is to demonstrate how these techniques can be used to enhance the predictive accuracy in many binary classification tasks, such as loan approval prediction. <br><br>\nThe dataset we have at hands consists of the following attributes:\n</span>","metadata":{}},{"cell_type":"markdown","source":"<table style=\"font-family: Arial, sans-serif; font-size: 16px;\">\n  <tr>\n    <th><b>Attribute</b></th>\n    <th><b>Description</b></th>\n  </tr>\n  <tr>\n    <td><b>loan_id</b></td>\n    <td>The unique identification number of each sample.</td>\n  </tr>\n  <tr>\n    <td><b>no_of_dependents</b></td>\n    <td>The number of dependents of the applicant.</td>\n  </tr>\n  <tr>\n    <td><b>education</b></td>\n    <td>The edducation level of the applicant, either <b>Graduate</b> or <b>Not Graduate</b>.</td>\n  </tr>\n  <tr>\n      <td><b>self_employed</b></td>\n      <td>Either if the applicant is self employed or not.</td>\n    </tr>\n  <tr>\n      <td><b>income_annum</b></td>\n      <td>The annual income of the applicant.</td>\n    </tr>\n  <tr>\n      <td><b>loan_amount</b></td>\n      <td>The total amount requested for the loan.</td>\n    </tr>\n  <tr>\n      <td><b>loan_term</b></td>\n      <td>The duration, in years, within which the loan must be repaid.</td>\n    </tr>\n  <tr>\n      <td><b>cibil_score</b></td>\n      <td>Credit score of the applicant.</td>\n    </tr>\n  <tr>\n      <td><b>residential_assets_value</b></td>\n      <td>The total value of the applicant's residential assets.</td>\n    </tr>\n  <tr>\n      <td><b>commercial_assets_value</b></td>\n      <td>The total value of the applicant's commercial assets.</td>\n    </tr>\n <tr>\n      <td><b>luxury_assets_value</b></td>\n      <td>The total value of the applicant's luxury assets.</td>\n    </tr>\n <tr>\n      <td><b>bank_asset_value</b></td>\n      <td>The total value of the applicant's bank assets.</td>\n    </tr>\n <tr>\n      <td><b>loan_status</b></td>\n      <td>Target variable. Describes whether the loan was approved or not.</td>\n    </tr>\n</table>","metadata":{}},{"cell_type":"markdown","source":"<span style=\"font-size: 20px\">Let's start by importing all the necessary libraries and defining some helpful functions.\n</span>","metadata":{}},{"cell_type":"code","source":"# Importing Libraries\n\n# Data Handling\nimport pandas as pd\nimport numpy as np\n\n# Financial Data Analysis\nimport yfinance as yf\nimport ta\nimport quantstats as qs\n\n# Data Visualization\nimport plotly.express as px\nimport plotly.graph_objs as go\nimport plotly.subplots as sp\nfrom plotly.subplots import make_subplots\nimport plotly.figure_factory as ff\nimport plotly.io as pio\nfrom IPython.display import display\nfrom plotly.offline import init_notebook_mode\ninit_notebook_mode(connected=True)\n\n# Statistics & Mathematics\nimport scipy.stats as stats\nimport statsmodels.api as sm\nfrom scipy.stats import shapiro, skew, anderson\nimport math\n\n# Feature Selection\nfrom sklearn.feature_selection import RFECV\n\n# Machine Learning Pipeline\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.base import BaseEstimator, TransformerMixin\n\n# Preprocessing data\nfrom sklearn.preprocessing import RobustScaler, StandardScaler, QuantileTransformer, FunctionTransformer\n\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.base import BaseEstimator, TransformerMixin\n\n# Model Selection for Cross Validation\nfrom sklearn.model_selection import StratifiedKFold, KFold, train_test_split,TimeSeriesSplit\n\n# Machine Learning metrics\nfrom sklearn.metrics import (\n    mean_squared_error,\n    r2_score,\n    mean_absolute_error,\n    cohen_kappa_score,\n    make_scorer,\n    roc_curve,\n    auc,\n    accuracy_score,\n    f1_score,\n    precision_score,\n    recall_score,\n    confusion_matrix\n)\n\n# ML regressors\nfrom sklearn.linear_model import HuberRegressor,RANSACRegressor, TheilSenRegressor\nfrom sklearn.ensemble import (\n    HistGradientBoostingRegressor, StackingRegressor, \n    AdaBoostRegressor, RandomForestRegressor, \n    GradientBoostingRegressor, StackingRegressor, VotingRegressor\n    )\nfrom catboost import CatBoostRegressor\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\n\n# ML classifiers\nfrom sklearn.ensemble import (\n    HistGradientBoostingClassifier, AdaBoostClassifier, \n    RandomForestClassifier, GradientBoostingClassifier,\n    StackingClassifier, VotingClassifier\n    )\nfrom sklearn.tree import DecisionTreeClassifier\nfrom catboost import CatBoostClassifier\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\n\n#Clustering algorithm\nfrom sklearn.cluster import KMeans\n\n# Fine-tuning \nimport optuna\n\n# Randomizer\nimport random\n\n# Encoder of categorical variables\nfrom sklearn.preprocessing import OrdinalEncoder, OneHotEncoder\n\n# Hiding warnings \nimport warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"_kg_hide-input":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"seed = 123 # Defining seed\nseaborn = 'seaborn' # Defining Plotly Template","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def dataframe_description(df):\n    \"\"\"\n    This function prints some basic info on the dataset.\n    \"\"\"\n    categorical_features = []\n    continuous_features = []\n    binary_features = []\n    \n    for col in df.columns:\n        if df[col].dtype == object:\n            categorical_features.append(col)\n        else:\n            if df[col].nunique() <= 2:\n                binary_features.append(col)\n            else:\n                continuous_features.append(col)\n    \n    print(\"\\n{} shape: {}\".format(type(df).__name__, df.shape))\n    print(\"\\n{:,.0f} samples\".format(df.shape[0]))\n    print(\"\\n{:,.0f} attributes\".format(df.shape[1]))\n    print(f'\\nMissing Data: \\n')\n    print(df.isnull().sum())\n    print(f'\\nDuplicates: {df.duplicated().sum()}')\n    print(f'\\nData types: \\n')\n    print(df.dtypes)\n    print(f'\\nCategorical features: \\n')\n    if len(categorical_features) == 0:\n        print('No Categorical Features')\n    else:\n        for feature in categorical_features:\n            print(feature)\n    print(f'\\nContinuous features: \\n')\n    if len(continuous_features) == 0:\n        print('No Continuous Features')\n    else:\n        for feature in continuous_features:\n            print(feature)\n    print(f'\\nBinary features: \\n')\n    if len(binary_features) == 0:\n        print('No Binary Features')\n    else:\n        for feature in binary_features:\n            print(feature)\n    print(f'\\n{type(df).__name__} Head: \\n')\n    display(df.head(5))\n    print(f'\\n{type(df).__name__} Tail: \\n')\n    display(df.tail(5))","metadata":{"_kg_hide-input":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_correlation(df):\n    '''\n    This function is resposible to plot a correlation map among features in the dataset\n    '''\n    corr = np.round(df.corr(), 2)\n    mask = np.triu(np.ones_like(corr, dtype = bool))\n    c_mask = np.where(~mask, corr, 100)\n\n    c = []\n    for i in c_mask.tolist()[1:]:\n        c.append([x for x in i if x != 100])\n\n\n    \n    fig = ff.create_annotated_heatmap(z=c[::-1],\n                                      x=corr.index.tolist()[:-1],\n                                      y=corr.columns.tolist()[1:][::-1],\n                                      colorscale = 'Magenta')\n\n    fig.update_layout(title = {'text': '<b>&nbsp;&nbsp;&nbsp;Feature Correlation <br><sup>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<i>Heatmap</i></sup></b>',\n                                'x': 0, 'xanchor': 'left'},\n                    margin = dict(t=210, l = 110),\n                    yaxis = dict(autorange = 'reversed', showgrid = False),\n                    xaxis = dict(showgrid = False),\n                    plot_bgcolor = '#F6F5F5',\n                    paper_bgcolor = '#F6F5F5',\n                    height = 950, width = 950)\n                     \n\n    fig.add_trace(go.Heatmap(z = c[::-1],\n                             colorscale = 'Magenta',\n                             showscale = True,\n                             visible = False))\n    fig.data[1].visible = True\n\n    for i in range(len(fig.layout.annotations)):\n        fig.layout.annotations[i].font.size = 12\n\n    fig.show()","metadata":{"_kg_hide-input":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def histogram_boxplot(df, feat):\n\n    '''\n    This function plots a Histogram and a Box Plot side by side\n    '''\n\n    fig = make_subplots(\n        rows=1,\n        cols=2,\n        subplot_titles=[\"Box Plot\", \"Histogram\"],\n        horizontal_spacing=0.2\n    )\n\n    fig.add_trace(go.Histogram(x=df[feat], name=\"Histogram\", marker_color = 'navy'), row=1, col=2)\n    fig.add_trace(go.Box(y=df[feat], name=\"Box Plot\", line_color = 'magenta'), row=1, col=1)\n\n    fig.update_layout(title = {'text': f'<b>&nbsp;&nbsp;Box Plot & Histogram<br><sup><i>&nbsp;&nbsp;&nbsp;&nbsp;{feat}</i></sup></b>',\n                                'x': 0, 'xanchor': 'left'},\n                    template = seaborn,\n                    margin = dict(t = 100),\n                    showlegend = False,\n                    plot_bgcolor = '#F6F5F5',\n                    paper_bgcolor = '#F6F5F5',\n                    height = 450, width = 1000\n                )\n    \n    fig.update_yaxes(title_text=f\"<b>{feat}</b>\", row=1, col=1)\n    fig.update_xaxes(title_text=\"<b>Values</b>\", row=1, col=1)\n    \n    fig.update_yaxes(title_text=\"<b>Frequency</b>\", row=1, col=2)\n    fig.update_xaxes(title_text=f\"<b>{feat}</b>\", row=1, col=2)\n\n    fig.show()","metadata":{"_kg_hide-input":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_model_performance(model_name, y_test, y_pred):\n\n    '''\n    This function is used to evaluate Classification models.\n    It plots an AUC-ROC Curve and a Confusion Matrix.\n    '''\n\n    # Computing ROC curve and ROC area\n    fpr, tpr, _ = roc_curve(y_test, y_pred)\n    roc_auc = auc(fpr, tpr)\n    \n    # Creating traces\n    trace0 = go.Scatter(x=fpr, y=tpr, mode='lines', \n                        name='AUC Score = %0.4f' % roc_auc, \n                        fill='tozeroy',\n                        line=dict(color='pink'))\n    trace1 = go.Scatter(x=[0, 1], y=[0, 1], mode='lines', \n                    name='Random Guess',\n                    line=dict(color='navy', dash='dash'))\n    \n    # Computing confusion matrix\n    cm = confusion_matrix(y_test, y_pred.round())\n    trace2 = go.Heatmap(z=cm, \n                        x=['Predicted 0', 'Predicted 1'], \n                        y=['True 0', 'True 1'], \n                        showscale=False, \n                        colorscale=[\n                            [0.0, \"#041d4f\"],  # Light blue\n                            [0.5, \"#365799\"],  # Medium blue\n                            [1.0, \"#8cb1fa\"],  # Dark purple\n                        ],\n                        xgap=20,\n                        ygap=20,\n                        text=cm,\n                        texttemplate=\"%{text}\")\n\n    # Creating subplots\n    fig = make_subplots(rows=1, cols=2, subplot_titles=('ROC Curve', 'Confusion Matrix'),\n                        horizontal_spacing=0.2)\n\n    # Adding traces to the subplots\n    fig.add_trace(trace0, row=1, col=1)\n    fig.add_trace(trace1, row=1, col=1)\n    fig.add_trace(trace2, row=1, col=2)\n\n    # Updating axes labels\n    fig.update_yaxes(title_text=\"True Positive Rate\", row=1, col=1)\n    fig.update_xaxes(title_text=\"False Positive Rate\", row=1, col=1)\n\n    # Updating overall layout\n    fig.update_layout(title={'text': f'<b>&nbsp;&nbsp;&nbsp;Logistic Regression ROC Curve and Confusion Matrix<br></b>',\n                             'x': 0, 'xanchor': 'left'},\n                      template='seaborn',\n                      margin=dict(t=100),\n                      showlegend=True,\n                      plot_bgcolor='#F6F5F5',\n                      paper_bgcolor='#F6F5F5',\n                      height=450, width=950)\n    \n    fig.show()","metadata":{"_kg_hide-input":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def pie_plot(df, feat):\n\n    '''\n    This function plots a pie plot.\n    '''\n\n    # Plotting pie plot \n    fig = px.pie(df, names = df[feat].values, hole = .75)\n    fig.update_traces(pull = [0.05] * len(df))\n    fig.update_traces(marker=dict(colors=['navy', 'magenta']))\n\n    # Layout\n    fig.update_layout(title = {'text':f'&nbsp;&nbsp;<b>Distribution of Classes<br><sup>&nbsp;&nbsp;&nbsp;&nbsp;<i>{feat}</i></sup></b>',\n                                'x': 0, 'xanchor': 'left'},\n                    margin=dict(t=100, r = 0),\n                    template = seaborn,\n                    showlegend = True,\n                    height = 550, width = 950,\n                    paper_bgcolor = '#F6F5F5'\n                    )\n    fig.show()","metadata":{"_kg_hide-input":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_highly_correlated_pairs(df, hue):\n\n    # Mapping colors and symbols\n    color_map = {\n        ' Approved': 'navy', \n        ' Rejected': 'magenta'   \n    }\n\n    symbol_map = {\n        ' Approved': 'circle',\n        ' Rejected': 'diamond'\n    }\n\n\n    df['symbol'] = df[' loan_status'].map(symbol_map)\n    \n    corr_matrix = np.round(df.corr(), 2)\n\n    # Iterating through the correlation matrix to find highly correlated pairs\n    for i in range(len(corr_matrix)):\n        for j in range(i+1, len(corr_matrix)):\n            if abs(corr_matrix.iloc[i, j]) > 0.60: # At least ‚àì 0.6 of correlation value\n                feat_x = corr_matrix.columns[i]\n                feat_y = corr_matrix.columns[j]\n\n                # Plotting scatterplots for each highly correlated pair\n                fig = px.scatter(df, x=feat_x, y=feat_y, color=' loan_status',\n                                 symbol='symbol',\n                                 labels={'loan_status': 'Loan Status'},\n                                 color_discrete_map=color_map,\n                                 opacity = 0.825,\n                                 template= seaborn)\n\n                fig.update_layout(title= {'text': f'<b>&nbsp;&nbsp;Scatterplot of Highly Correlated Features<br><sup><i>&nbsp;&nbsp;&nbsp;&nbsp;{feat_x} x {feat_y}</i></sup></b>',\n                                'x': 0, 'xanchor': 'left'},\n                                  showlegend=True,\n                                  plot_bgcolor='#F6F5F5',\n                                  paper_bgcolor='#F6F5F5',\n                                  height=550, width=950)\n                \n                fig.show()","metadata":{"_kg_hide-input":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def split_train_test(df, test_size, seed):\n    \n    '''\n    This function splits a dataframe for training and testing according to test_size\n    '''\n    \n    train, test = train_test_split(df, test_size = test_size, shuffle = True, random_state = seed) # Splitting data\n\n    print(f'\\n Train shape: {train.shape}\\n')\n    print(f'\\n {len(train):,.0f} Samples \\n')\n    print(f'\\n {len(train.columns)} Attributes \\n')\n    display(train.head(10))\n    print('\\n' * 2)\n\n    print(f'\\n Test shape: {test.shape:}\\n')\n    print(f'\\n {len(test):,.0f} Samples \\n')\n    print(f'\\n {len(test.columns)} Attributes \\n')\n    display(test.head(10))\n    \n    return train, test","metadata":{"_kg_hide-input":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def X_y_split(df, target_variable):\n    \n    '''\n    This function takes a dataframe and a target variable to create an X (predictors) dataframe and a y Series\n    '''\n    \n    X, y = df.drop([target_variable], axis = 1), df[target_variable] \n\n    #Printing info on X and y\n    print(f'\\nX shape: {X.shape}\\n')\n    print(f'\\n{len(X):,.0f} Samples \\n')\n    print(f'\\n{len(X.columns)} Attributes \\n')\n    display(X.head(10))\n    print('\\n')\n    print(f'\\ny shape: {y.shape}\\n')\n    print(f'\\n{len(y):,.0f} Samples \\n')\n    display(y.head(10))\n    \n    return X, y","metadata":{"_kg_hide-input":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def perform_shapiro_wilk_test(df):\n\n    '''\n    This function performs a Shapiro-Wilk test to check if the attributes are normally distributed. \n    It then generates a DataFrame consisting of Feature Name, Test Statistcis, P-Values & a Conclusion str.\n\n    Null Hypothesis: The sample comes from a normally distributed population\n\n    If p-value is less than 0.05, we reject the null hypothesis\n\n    '''\n\n    results = []\n\n    for feature in continuous_features:\n        stats, p_value = shapiro(df[feature])\n        skewness = np.round(skew(df[feature]), 2)\n\n        if p_value < 0.05:\n            conclusion = 'Not Normally Distributed'\n        else:\n            conclusion = 'Normally Distributed'\n\n        # Append the results to the list\n        results.append([feature, stats, p_value, skewness, conclusion])\n\n    # Convert the results into a DataFrame\n    results_df = pd.DataFrame(results, columns=['Feature', 'Shapiro-Wilk Statistic', 'P-Value', 'Skewness', 'Conclusion'])\n\n    print(\"\\n\\033[1m\" + \"* * * Shapiro-Wilk Test Completed * * *\" + \"\\033[0m\\n\")\n\n    return results_df","metadata":{"_kg_hide-input":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_woe(df):\n        \n      '''\n      This function plots a barplot consisting of WoE values for each Bin of a certain feature.\n      '''\n      \n      fig = px.bar(df, x='Bin Values', y='WoE',\n                   color_discrete_sequence = ['navy'])\n      fig.update_layout(\n            xaxis_title='Bins',\n            yaxis_title='Weight of Evidence (WoE)',\n            template= seaborn\n        )\n\n      fig.update_layout(title= {'text': f'<b>&nbsp;&nbsp;&nbsp;Weight of Evidence<br><sup><i>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;loan_term_bin</i></sup></b>',\n                                'x': 0, 'xanchor': 'left'},\n                                  showlegend=True,\n                                  plot_bgcolor='#F6F5F5',\n                                  paper_bgcolor='#F6F5F5',\n                                  height=550, width=950)\n        \n      fig.show()","metadata":{"_kg_hide-input":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1 id = 'eda' style=\"border-bottom: 1px solid #ccc;\n                        padding-bottom: 10px;\n                        margin-bottom: 10px;\n                        font-size: 38px;\n                        font-weight: bold;\n                        color: black;\n                        font-family: Poppins\">Exploratory Data Analysis (EDA)</h1>","metadata":{}},{"cell_type":"markdown","source":"<span style=\"font-size: 20px\"> Before we approach the techniques mentioned in the Introduction session of this notebook, we will conduct an extensive <mark><b>Exploratory Data Analysis</b></mark> to better understand the data we have at hands.\n</span>","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv('loan_approval_dataset.csv') # Loading dataet\ndataframe_description(df) # Printing information on the dataset","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> <span style=\"font-size: 20px\"><b>üìù We have a total of 13 attributes and 4,269 samples to work with.<br><br>\n    üìù  There are no missing values in this datasframe.<br><br>\n    üìù No duplicates.<br><br>\n    üìù We have a bunch of continuous features to check for WoE and IV.</b></span>","metadata":{}},{"cell_type":"code","source":"df_copy = df.copy() # Creating a copy of the original dataset\ndf_copy = df_copy.drop('loan_id', axis = 1) # Removing 'Id' column","metadata":{"_kg_hide-input":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"continuous_features = [col for col in df_copy.columns if df_copy[col].dtype in ('int64', 'float64')] # Creating a list of continuous features\n\n# Histogram and Box Plot for each continuous variable\nfor feat in continuous_features:\n    histogram_boxplot(df_copy, feat)","metadata":{"_kg_hide-input":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> <span style=\"font-size: 20px\"><b>üìù The histograms suggest that none of these features is normally distributed.<br><br>\n    üìù  We have some outliers in <code>residential_assets_values</code>, <code>commercial_assets_values</code>, and <code>bank_asset_value</code>. These features are also positively skewed, indicating a larger number of observations with \"lower\" assets values and a minority of observations owning much higher assets values.<br><br>\n    üìù  On average, the observations in the dataset have <mark>3 dependents</mark>, which are people who financially depend on the person applying for a loan. <br><br>\n    üìù  On average, the annual income of all observations is at around <mark>USD 5 million</mark> mark, suggesting a higher-income profile for most of these clients applying for a loan. The loan amount average at the <mark>USD 14 million</mark> mark, with an average loan term of <mark>10 years</mark>, which is the period in which the loan must be repaid.</b><span>","metadata":{}},{"cell_type":"code","source":"pie_plot(df_copy, ' loan_status') # Distribution of target variable","metadata":{"_kg_hide-input":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> <span style=\"font-size: 20px\"><b>üìù Overall, the majority of observations had their loan application <mark>approved</mark>, with only 37.8% of them being denied due to the risk of <code>default</code>. </b><span>","metadata":{}},{"cell_type":"code","source":"plot_correlation(df_copy) # Correlation plot","metadata":{"_kg_hide-input":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> <span style=\"font-size: 20px\"><b>üìù The feature correlation plot shows a large amount of high correlations between many features, such as <code>loan_amount</code> and <code>income_annum</code>, for instance. With a Pearson's R correlation of 0.93, this suggests that people with higher annual income may apply for higher loan amounts.</b> <span>","metadata":{}},{"cell_type":"markdown","source":"<span style=\"font-size: 20px\"> Due to the high positive correlations, let's plot some scatterplots of the highly-correlated pairs colored by whether they request was approved or rejected. This may help us in identifying the behavior of observations, and patterns that may indicate the profile of clients that were rejected and approved.\n</span>","metadata":{}},{"cell_type":"code","source":"plot_highly_correlated_pairs(df_copy, ' loan_status') # Scatterplots ","metadata":{"_kg_hide-input":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> <span style=\"font-size: 20px\"><b>üìù It's possible to see that the loan amount increases as the annual income, luxury assets value, and the bank assets value also increase. This implies that higher-income clients apply for higher loans. <br><br>\nüìù It's also possible to see that the higher the annual income, the higher the overall assets value. <br><br>\nüìù Overall, loans saw approval and rejection across different income brackets. Even high-earners and individuals with substantial assets faced denials. However, it seems to exist some sort of spike in rejections among those applicants with lower-income and lower-asset.</b> <span>","metadata":{}},{"cell_type":"code","source":"perform_shapiro_wilk_test(df_copy)","metadata":{"_kg_hide-input":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> <span style=\"font-size: 20px\"><b>üìù The <mark>Shapiro-Wilk test</mark> confirms the non-normality of distributions.</b><span>","metadata":{}},{"cell_type":"markdown","source":"<h1 id = 'woe_and_iv' style=\"border-bottom: 1px solid #ccc;\n                        padding-bottom: 10px;\n                        margin-bottom: 10px;\n                        font-size: 38px;\n                        font-weight: bold;\n                        color: black;\n                        font-family: Poppins\">Weight of Evidence & Information Value</h1>","metadata":{}},{"cell_type":"markdown","source":"<p style=\"font-size: 20px\"> The <b>Weight of Evidence</b> and <b>Information Value</b> are concepts that have been present in <mark><b>Logistic Regression</b></mark> for decades, specially in the credit scoring field. These have been used particularly for allowing easier <mark>interpretability</mark> by identifying the most relevant features that describe an event, specially in the case of credit default. <br><br>\nThe Weight of Evidence is given by the following formula: <br><br><br></p>\n\n<p style=\"font-size: 24px\">\n\\[\n\\text{WoE} = \\ln( \\frac{\\text{Proportion of Good Outcomes}}{\\text{Proportion of Bad Outcomes}})\n\\]\n<br><br></p>\n<p style=\"font-size: 20px\"> where: <br><br>\n‚Ä¢ $Proportion$ $of$ $Good$ $Outcomes$ is the % of clients with lower risk of default.<br><br>\n‚Ä¢ $Proportion$ $of$ $Bad$ $Outcomes$ is the % of clients with higher risk of default.<br><br>\n‚Ä¢ $ln$ is the natural log.<br><br><br>\nThe Weight of Evidence is very effective in dealing with <mark>outliers</mark> and <mark>missing values</mark>. It is computed by binning continuous features into groups of smaller bins, in which: <br><br><br>\n‚Ä¢ Each bin must have at least 5% of samples.<br><br>\n‚Ä¢ WoE values must be monotonic, either growing or decreasing in value according to the bins.<br><br>\n‚Ä¢ Missing values must be binned separately.<br><br><br>\nFor the Information Value, the following formula is used: <br><br><br></p>\n<p style=\"font-size: 18px\">\n\\[\\text{IV} = \\sum_{i=1}^{n} \\left( \\text{Proportion of Good Outcomes}_i - \\text{Proportion of Bad Outcomes}_i \\right) \\times \\text{WoE}_i\n\\]\n<br><br></p>\n<p style=\"font-size: 20px\"> where: <br><br>\n‚Ä¢ $Proportion$ $of$ $Good$ $Outcomes$ is the % of clients with lower risk of default.<br><br>\n‚Ä¢ $Proportion$ $of$ $Bad$ $Outcomes$ is the % of clients with higher risk of default.<br><br>\n‚Ä¢ $\\sum$ is the sum of all the subtractions.<br><br><br>\nIt is essentially the Information Value that will indicate either a feature is useful or not for predicting the target variable, in which: <br><br><br>\n‚Ä¢ <b>IV</b> lower than 0.02 indicates that the feature is useless for predictions.<br><br>\n‚Ä¢ <b>IV</b> higher than 0.02 and lower than 0.1 indicates that the feature has weak predictive power.<br><br>\n‚Ä¢ <b>IV</b> higher than 0.1 and lower than 0.3 indicates that the feature has medium predictive power.<br><br>\n‚Ä¢ <b>IV</b> higher than 0.3 and lower than 0.5 indicates that the feature has strong predictive power.<br><br>\n‚Ä¢ <b>IV</b> higher than 0.5 may be suspicious, either too good to be true.<br><br>\n</p>","metadata":{}},{"cell_type":"markdown","source":"<span style=\"font-size: 20px\"> Let's start our experiment by encoding the target variable <code>loan_status</code>. Since we want to identify those customers that were deemed higher risk for default, let's label <code>Rejected</code> as 1 and <code>Approved</code> as 0. Then, we will split the dataframe into training and testing sets.\n</span>","metadata":{}},{"cell_type":"code","source":"# Encoding target variable \ndf[' loan_status'] = df[' loan_status'].replace({\n    ' Approved': 0, # Approved = Good Event = 0\n    ' Rejected': 1  # Rejected = Bad Event = 1\n})\ndf # Visualizing results","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train, test = split_train_test(df, 0.3, seed) # Splitting 70% of data for training and 30% for validation","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = train.drop('loan_id', axis = 1) # Dropping 'ID' column\n\n# Encoding categorical features\ntrain[' education'] = train[' education'].replace({\n    ' Not Graduate': 0, ' Graduate': 1\n})\n\ntrain[' self_employed'] = train[' self_employed'].replace({\n    ' No': 0, ' Yes': 1\n})","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Selecting columns to bin\ncolumns_to_bin = [' no_of_dependents',' income_annum',' loan_amount', ' loan_term', \n                  ' cibil_score',' residential_assets_value', ' commercial_assets_value',\n                  ' luxury_assets_value', ' bank_asset_value']","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<span style=\"font-size: 20px\"> Below, we will create the bins for the continuous features. It's important to define <code>bin_edges</code>, so we ensure that the bins will be similar throughout the training and testing sets. \n</span>","metadata":{}},{"cell_type":"code","source":"bin_edges_dict = {} # Creating empty dictionaire \n\n# Splitting samples into different bins and maintaining the edges for reproducibility\nfor col in columns_to_bin:\n    train[col + '_bin'], bin_edges = pd.qcut(train[col], q=20, duplicates='drop', retbins=True)\n    bin_edges_dict[col] = bin_edges","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train # Visualizing results","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<span style=\"font-size: 20px\"> Even though there are widely used packages for <i>R</i> that can easily compute the Weight of Evidence, the same is not quite true for <i>Python</i>. For this reason, I have used a function that is available on the <a href = \"http://www.sanaitics.com/UploadedFiles/html_files/1770WoE_RvsPython.html\">Sanaitics</a> website.\n</span>","metadata":{}},{"cell_type":"code","source":"# Function to compute Weight of Evidence\n# Source: http://www.sanaitics.com/UploadedFiles/html_files/1770WoE_RvsPython.html\n\ndef calculate_woe_iv(dataset, feature, target):\n    lst = []\n    for i in range(dataset[feature].nunique()):\n        val = list(dataset[feature].unique())[i]\n        lst.append({\n            'Bin Values': val,\n            'All': dataset[dataset[feature] == val].count()[feature],\n            'Good': dataset[(dataset[feature] == val) & (dataset[target] == 0)].count()[feature],\n            'Bad': dataset[(dataset[feature] == val) & (dataset[target] == 1)].count()[feature]\n        }) \n    dset = pd.DataFrame(lst)\n    dset['Distr_Good'] = dset['Good'] / dset['Good'].sum()\n    dset['Distr_Bad'] = dset['Bad'] / dset['Bad'].sum()\n    dset['WoE'] = np.log(dset['Distr_Good'] / dset['Distr_Bad'])\n    dset = dset.replace({'WoE': {np.inf: 0, -np.inf: 0}})\n    dset['IV'] = (dset['Distr_Good'] - dset['Distr_Bad']) * dset['WoE']\n    iv = dset['IV'].sum()\n    dset = dset.sort_values(by='WoE')\n    return dset, iv","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Dropping original columns. Let's keep only the binned columns\ntrain_bins = train.drop([' no_of_dependents',' income_annum',\n       ' loan_amount', ' loan_term', ' cibil_score',\n       ' residential_assets_value', ' commercial_assets_value',\n       ' luxury_assets_value', ' bank_asset_value'], axis = 1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_bins # Visualizing results","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Computing WoEs and IVs\n\nlst = [] # List that will store WoE values for each bin\nIV_df = pd.DataFrame(columns=['Variable','IV']) # Dataframe that will store Information Value for each feature\nfor col in train_bins.columns:\n    if col == ' loan_status': continue # Ignore target variable\n    else:\n        df, iv = calculate_woe_iv(train_bins, col, ' loan_status')\n               \n    lst.append(df)\n    IV_df = IV_df.append({\n                \"Variable\" :col ,\n                \"IV\" : iv,\n                },ignore_index=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Defining threshold values for IVs\ndef interpret_iv(iv_value):\n    if iv_value < 0.02:\n        return 'Does not appear to be useful for prediction'\n    elif iv_value < 0.1:\n        return 'Weak predictive power'\n    elif iv_value < 0.3:\n        return 'Medium predictive power'\n    else:\n        return 'Strong predictive power'","metadata":{"_kg_hide-input":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"display(IV_df) # Displaying dataframe\n\n# Iterating through samples in the datafrane above and labeling their IVs\nfor index, row in IV_df.iterrows():\n    feature = row['Variable']\n    iv_value = row['IV']\n    status = interpret_iv(iv_value)\n    print(f\"{feature}: {status}\")","metadata":{"_kg_hide-input":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> <span style=\"font-size: 20px\"><b>üìù It appears that the most relevant features for predicting if a loan was accepted or rejected is the <mark>term</mark>, in years, in which the loan is supposed to be repaid, as well as the <mark>credit score</mark> of the applicant. </b><span>","metadata":{}},{"cell_type":"markdown","source":"<span style=\"font-size: 20px\"> Below, we can see the WoE values for each bin of each variable.\n</span>","metadata":{}},{"cell_type":"code","source":"# Printing WoE values for each Bin of each feature\nfor df, (index, row) in zip(lst, IV_df.iterrows()):\n    feature_name = row['Variable']\n    if feature_name == ' loan_status':\n        continue\n    print(f\"\\nFeature: {feature_name}\\n\")\n    print(df[['Bin Values', 'WoE']])\n    print(\"\\n\")","metadata":{"_kg_hide-input":true,"_kg_hide-output":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<span style=\"font-size: 20px\"> It's important to reinforce that the WoE values must be monotonic, either increasing or decreasing per each bin. We can plot the WoE values for the <code>loan_term_bin</code> to check if the WoE values follow this expected behavior.\n</span>","metadata":{}},{"cell_type":"code","source":"# Creating a new dataframe containing the WoE label for each Bin\nwoe_df = pd.DataFrame(columns= ['Bin Values', 'WoE']) \nwoe_df['Bin Values'] = ['(1.999, 4.0]', '(12.0, 14.0]', '(16.0, 18.0]', \n                        '(10.0, 12.0]', '(18.0, 20.0]', '(6.0, 8.0]', \n                        '(14.0, 16.0]','(4.0, 6.0]', '(8.0, 10.0]']\n\nwoe_df['WoE'] = [-0.915108, -0.010079, 0.071173, 0.134383, 0.167994, 0.185639, 0.242751, 0.288206, 0.348493]\nwoe_df","metadata":{"_kg_hide-input":true,"_kg_hide-output":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_woe(woe_df)","metadata":{"_kg_hide-input":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> <span style=\"font-size: 20px\"><b>üìù We can see that indeed, the WoE values increase according to each bin.</b> <span>","metadata":{}},{"cell_type":"markdown","source":"<span style=\"font-size: 20px\"> We now must filter our training and testing set, maintaining only the features deemed relevant for predicting <code>loan_status</code>, which in this case are <code>loan_term_bin</code> and <code>cibil_score_bin</code>. <br><br>\nWe must then replace the bins by their respective Weight of Evidence values, which will be then used for predicting the target variable.\n</span>","metadata":{}},{"cell_type":"code","source":"train = train[[' loan_term_bin', ' cibil_score_bin', ' loan_status']] # Selecting features for the train dataset\ntrain","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Creating dictionaire to replace Bin values by the respective WoE value\nloan_term_mapping = {\n    '(1.999, 4.0]': -0.915108,\n    '(12.0, 14.0]': -0.010079,\n    '(16.0, 18.0]':  0.071173,\n    '(10.0, 12.0]':  0.134383,\n    '(18.0, 20.0]':  0.167994,\n    '(6.0, 8.0]':  0.185639,\n    '(14.0, 16.0]':  0.242751,\n    '(4.0, 6.0]':  0.288206,\n    '(8.0, 10.0]':  0.348493\n}\n\ntrain[' loan_term_bin'] = train[' loan_term_bin'].astype(str) # Converting 'Category' data type to 'String' \ntrain[' loan_term_bin'] = train[' loan_term_bin'].replace(loan_term_mapping) # Mapping WoE values to the Bins","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Creating dictionaire to replace Bin values by the respective WoE value\ncibil_score_mapping = {\n    '(809.0, 839.0]': -4.476178,\n    '(575.15, 605.0]': -4.469489,\n    '(692.55, 718.0]': -4.469489,\n    '(718.0, 748.25]': -4.462755,\n    '(839.0, 869.65]': -4.435356,\n    '(779.0, 809.0]': -4.435356,\n    '(748.25, 779.0]': -3.809348,\n    '(664.0, 692.55]': -3.756002,\n    '(545.0, 575.15]': -1.210782,\n    '(869.65, 900.0]':  0.000000,\n    '(636.0, 664.0]':  0.000000,\n    '(605.0, 636.0]':  0.000000,\n    '(299.999, 331.0]':  2.200590,\n    '(456.0, 485.0]':  2.457553,\n    '(331.0, 361.7]':  2.576441,\n    '(424.0, 456.0]':  2.667143,\n    '(393.0, 424.0]':  2.716756,\n    '(485.0, 516.0]':  2.874783,\n    '(516.0, 545.0]':  3.071036,\n    '(361.7, 393.0]':  3.187699\n    }\n\ntrain[' cibil_score_bin'] = train[' cibil_score_bin'].astype(str) # Converting 'Category' data type to 'String' \ntrain[' cibil_score_bin'] = train[' cibil_score_bin'].replace(cibil_score_mapping) # Mapping WoE values to the Bins","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train # Visualizing training set with the WoE values ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"columns_to_bin = [' loan_term', ' cibil_score'] # Selecting columns to Bin\nfor col in columns_to_bin:\n    bin_edges = bin_edges_dict[col] \n    test[col + '_bin'] = pd.cut(test[col], bins=bin_edges, include_lowest=True)\n\ntest = test[[' loan_term_bin', ' cibil_score_bin', ' loan_status']] # Selecting features for the test dataset\ntest","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Mapping WoE values to the test dataset\ntest[' loan_term_bin'] = test[' loan_term_bin'].astype(str)\ntest[' cibil_score_bin'] = test[' cibil_score_bin'].astype(str)\n\ntest[' loan_term_bin'] = test[' loan_term_bin'].replace(loan_term_mapping)\ntest[' cibil_score_bin'] = test[' cibil_score_bin'].replace(cibil_score_mapping)\ntest","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1 id = 'model' style=\"border-bottom: 1px solid #ccc;\n                        padding-bottom: 10px;\n                        margin-bottom: 10px;\n                        font-size: 38px;\n                        font-weight: bold;\n                        color: black;\n                        font-family: Poppins\">Modeling</h1>","metadata":{}},{"cell_type":"markdown","source":"<span style=\"font-size: 20px\"> After identifying the most relevant features and replacing their bin values by their respective WoE values, we are all set to modeling. <br><br>\nOne of the main goals behind the Weight of Evidence and Information Value, besides all other that were previously mentioned during the development of this notebook, is <mark>model interpretability</mark>. These tools are supposed to give us a straightforward reason why an observation was predicted as either 1 or 0. In the credit scoring industry, explainability is extremely important and that's why it's much preferred - and sometimes even required by regulations - to use <mark><b>White Box Models</b></mark>. <br><br>\nFor this reason, we are going to use a simple <b>Logistic Regression</b> model with the statsmodels library, allowing for a very straightforward model for predictions. It's also important to reinforce that by replacing our raw data by their Weight of Evidence values, we introduce <mark>linearity</mark> to the data, which is a very important characteristc for Logistic Regression. <br><br>\nLet's start by splitting the training and testing sets into $X$ and $y$ variables. \n</span>","metadata":{}},{"cell_type":"code","source":"X_train, y_train = X_y_split(train, ' loan_status') # Splitting training dataset","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_test, y_test = X_y_split(test, ' loan_status') # Splitting testing dataset","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Creating Logistic Regression Model\nmodel = sm.Logit(y_train,\n                 X_train)\nresult = model.fit() # Fitting model to the training data\n\n# Priting results\nprint(result.summary())","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> <span style=\"font-size: 20px\"><b>üìù With a Pseud R-squared equal to 0.6561, our model explains about 65.61% of the variability of the dependent variable. <br><br>\nüìù A Log-Likelihood of -676.85 is better than a null model with a Log-Likelihood of -1968.1. The closer to zero, the better. <br><br>\nüìù With a coefficient of 2.7165, higher values for <code>loan_term</code> increases the odds of having the loan request approved. <br><br>\nüìù With a coefficient of 1.1309, higher values for <code>cibil_score</code> increases the odds of having the loan request approved.<br><br>\nüìù Both the predictors have very lowe standar deviation errors, specially the <code>cibil_score</code>, meaning they are reliable. <br><br>\nüìù All the P-Values are below 0.05, which means that our model is overall statistically significant for predicting loan approval or rejection.</b><span>","metadata":{}},{"cell_type":"markdown","source":"<span style=\"font-size: 20px\"> Let's now run predictions on the testing set and plot the the results.\n</span>","metadata":{}},{"cell_type":"code","source":"y_pred = result.predict(X_test) # Running predictions on the testing data\nplot_model_performance(result, y_test, y_pred) # Plotting results","metadata":{"_kg_hide-input":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<span style=\"font-size: 20px\"> With an AUC score of 0.98, our simple <b>Logistic Regression</b> model with just <b>two predictors</b> is extremelly effective in detecting requests with higher risk of default (<code>1</code>). <br><br>\nThank you so much for reading!</span>","metadata":{}},{"cell_type":"markdown","source":"#### <hr style=\"border: 0; height: 1px; border-top: 0.85px solid #b2b2b2\">\n<div style=\"text-align: left; color: #8d8d8d; padding-left: 15px; font-size: 14.25px;\">\n    Luis Fernando Torres, 2023 <br><br>\n    Let's connect!üîó<br>\n    <a href=\"https://www.linkedin.com/in/luuisotorres/\">LinkedIn</a> ‚Ä¢ <a href=\"https://medium.com/@luuisotorres\">Medium</a> ‚Ä¢ <a href = \"https://huggingface.co/luisotorres\">Hugging Face</a><br><br>\n</div>","metadata":{}}]}